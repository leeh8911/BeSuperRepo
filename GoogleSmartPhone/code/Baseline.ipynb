{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references\n",
    "1. https://www.kaggle.com/columbia2131/device-eda-interpolate-by-removing-device-en-ja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.617434Z",
     "start_time": "2021-06-21T03:02:36.916070Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.723298Z",
     "start_time": "2021-06-21T03:02:55.618434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.738302Z",
     "start_time": "2021-06-21T03:02:55.724299Z"
    }
   },
   "outputs": [],
   "source": [
    "notebookName = 'Baseline'\n",
    "PATH = Path(f\"./models/{notebookName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.783301Z",
     "start_time": "2021-06-21T03:02:55.739301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(PATH):\n",
    "    dir_list = os.listdir(PATH)\n",
    "    num_files = 0\n",
    "    while True:\n",
    "        if os.path.isfile(str(PATH / f\"{num_files}\")):\n",
    "            print(num_files)\n",
    "            num_files += 1\n",
    "        else:\n",
    "            break\n",
    "else:\n",
    "    os.mkdir(PATH)\n",
    "    num_files = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.798302Z",
     "start_time": "2021-06-21T03:02:55.784299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.843299Z",
     "start_time": "2021-06-21T03:02:55.799300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models\\\\Baseline\\\\model - 4.pth',\n",
       " 'models\\\\Baseline\\\\param - 4.pth',\n",
       " 'models\\\\Baseline\\\\result - 4.csv')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 1990\n",
    "lr = 0.0005\n",
    "batch_size = 128\n",
    "EPOCH_NUM = 1000\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "experience_name = f\"{num_files}\"\n",
    "checkpoint_name = \"check_point\"\n",
    "model_name = str(\"model - \" + experience_name)\n",
    "param_name = str(\"param - \" + experience_name)\n",
    "result_name = str(\"result - \" + experience_name)\n",
    "\n",
    "dummy_path = str(PATH / f\"{num_files}\")\n",
    "checkpoint_path = str(PATH / f\"{checkpoint_name}.pth\")\n",
    "model_path = str(PATH / f\"{model_name}.pth\")\n",
    "param_path = str(PATH / f\"{param_name}.pth\")\n",
    "result_path = str(PATH / f\"{result_name}.csv\")\n",
    "model_path, param_path, result_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.858300Z",
     "start_time": "2021-06-21T03:02:55.844300Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    \n",
    "    c = 2 * np.arcsin(a ** 0.5)\n",
    "    dist = 6_367_000 * c\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.873300Z",
     "start_time": "2021-06-21T03:02:55.860299Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_score(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    output_df = input_df.copy()\n",
    "    \n",
    "    output_df['meter'] = input_df.apply(\n",
    "        lambda r: calc_haversine(\n",
    "            r.latDeg, r.lngDeg, r.t_latDeg, r.t_lngDeg\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    meter_score = output_df['meter'].mean()\n",
    "    print(f'error meter: {meter_score}')\n",
    "\n",
    "    scores = []\n",
    "    for phone in output_df['phone'].unique():\n",
    "        _index = output_df['phone']==phone\n",
    "        p_50 = np.percentile(output_df.loc[_index, 'meter'], 50)\n",
    "        p_95 = np.percentile(output_df.loc[_index, 'meter'], 95)\n",
    "        scores.append(p_50)\n",
    "        scores.append(p_95)\n",
    "\n",
    "    score = sum(scores) / len(scores)\n",
    "    print(f'score: {score}')\n",
    "    \n",
    "    return output_df, meter_score , score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:55.888300Z",
     "start_time": "2021-06-21T03:02:55.875301Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_score_np(predict:torch.Tensor, target:torch.Tensor):\n",
    "    m = []\n",
    "    predict = predict.detach().numpy()\n",
    "    target = target.detach().numpy()\n",
    "    for i in range(predict.shape[0]):\n",
    "        temp = calc_haversine(predict[i,0], predict[i,1], target[i,0], target[i,1])\n",
    "        m.append(temp)\n",
    "    \n",
    "    m = np.array(m)\n",
    "    score = (np.percentile(m, 50) + np.percentile(m, 95))/2\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T03:02:56.520756Z",
     "start_time": "2021-06-21T03:02:55.889302Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"../input/google-smartphone-decimeter-challenge\")\n",
    "df_train = pd.read_pickle(str(data_dir / \"gsdc_train.pkl.gzip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.945Z"
    }
   },
   "outputs": [],
   "source": [
    "# check score\n",
    "df_train, default_loss, default_meas = check_score(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## Simple view, what is in data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.959Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in df_train.columns:\n",
    "    print(c)\n",
    "    print(df_train[c].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.961Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.962Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[['latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM',\n",
    "             'xSatVelMps', 'ySatVelMps', 'zSatVelMps', \n",
    "             'xSatPosM', 'ySatPosM', 'zSatPosM',\n",
    "            'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec', \n",
    "            'DriftXRadPerSec', 'DriftYRadPerSec', 'DriftZRadPerSec']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.962Z"
    }
   },
   "outputs": [],
   "source": [
    "# ref: https://towardsdatascience.com/fast-and-robust-sliding-window-vectorization-with-numpy-3ad950ed62f5\n",
    "def extract_window(src:np.array, sequence_length = 16)->np.array:\n",
    "    output = []\n",
    "    for i in range(src.shape[0]):\n",
    "        index = np.arange(i-sequence_length, i, 1)\n",
    "        index[index < 0] = 0\n",
    "        \n",
    "        output.append(np.expand_dims(src[index,:], 0))\n",
    "            \n",
    "    output = np.concatenate(output, axis = 0)\n",
    "    return output\n",
    "    \n",
    "\n",
    "def extract_features(df, sequence_length = 16, train = True):\n",
    "    Xcols = ['latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM',\n",
    "             'xSatVelMps', 'ySatVelMps', 'zSatVelMps', \n",
    "             'xSatPosM', 'ySatPosM', 'zSatPosM',\n",
    "            'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec', \n",
    "            'DriftXRadPerSec', 'DriftYRadPerSec', 'DriftZRadPerSec']\n",
    "    Ycols = ['t_latDeg', 't_lngDeg', 't_heightAboveWgs84EllipsoidM', 'speedMps', 'courseDegree']\n",
    "    \n",
    "    X = df[Xcols]\n",
    "    if train:\n",
    "        y = df[Ycols]\n",
    "    else:\n",
    "        y = None\n",
    "        \n",
    "    X = extract_window(np.array(X), sequence_length = sequence_length)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = extract_features(df_train)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.980Z"
    }
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 1024)\n",
    "        self.fc5 = nn.Linear(1024, output_size)\n",
    "        \n",
    "        self.dropout_30 = nn.Dropout(p = 0.3)\n",
    "        self.dropout_60 = nn.Dropout(p = 0.6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        \n",
    "        x =  F.relu(self.fc1(x))\n",
    "        x = self.dropout_60(x)\n",
    "        x =  F.relu(self.fc2(x))\n",
    "        x = self.dropout_60(x)\n",
    "        x =  F.relu(self.fc3(x))\n",
    "        x = self.dropout_30(x)\n",
    "        x =  F.relu(self.fc4(x))\n",
    "        x = self.dropout_30(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = SimpleNetwork(X.shape[1] * X.shape[2], y.shape[1])\n",
    "model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/7, random_state = 1990)\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size = batch_size, shuffle = True)\n",
    "loader_test = DataLoader(ds_test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.982Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.983Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()  # 신경망을 학습 모드로 전환\n",
    "\n",
    "    # 데이터로더에서 미니배치를 하나씩 꺼내 학습을 수행\n",
    "    predict = []\n",
    "    ground = []\n",
    "    for data, targets in loader_train:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 경사를 0으로 초기화\n",
    "        outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "        loss = loss_func(outputs, targets)  # 출력과 훈련 데이터 정답 간의 오차를 계산\n",
    "        loss.backward()  # 오차를 역전파 계산\n",
    "        optimizer.step()  # 역전파 계산한 값으로 가중치를 수정\n",
    "        \n",
    "        predict.append(outputs)\n",
    "        ground.append(targets)\n",
    "\n",
    "    # 정확도 출력\n",
    "    predict = torch.cat(predict,axis = 0)\n",
    "    ground = torch.cat(ground,axis = 0)\n",
    "    \n",
    "    loss = loss_func(predict, ground)\n",
    "    meas = check_score_np(predict.to('cpu'), ground.to('cpu'))\n",
    "    return loss, meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.983Z"
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # 신경망을 추론 모드로 전환\n",
    "\n",
    "    # 데이터로더에서 미니배치를 하나씩 꺼내 추론을 수행\n",
    "    predict = []\n",
    "    ground = []\n",
    "    with torch.no_grad():  # 추론 과정에는 미분이 필요없음\n",
    "        for data, targets in loader_test:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "            predict.append(outputs)\n",
    "            ground.append(targets)\n",
    "\n",
    "    # 정확도 출력\n",
    "    predict = torch.cat(predict,axis = 0)\n",
    "    ground = torch.cat(ground,axis = 0)\n",
    "    \n",
    "    loss = loss_func(predict, ground)\n",
    "    meas = check_score_np(predict.to('cpu'), ground.to('cpu'))\n",
    "    return loss, meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.984Z"
    }
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "check_meas = np.inf\n",
    "check_loss = np.inf\n",
    "check_epoch = 0\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    train_loss, train_meas = train(epoch)\n",
    "    test_loss, test_meas = test()\n",
    "    \n",
    "    history.append({'epoch':epoch, 'train_loss':train_loss, 'train_meas':train_meas, 'test_loss':test_loss, 'test_meas':test_meas})\n",
    "    \n",
    "    if (test_meas < check_meas):\n",
    "        print(\"\")\n",
    "        print(f\"/***CHECK_POINT***/ \")\n",
    "        print(f\"TRAIN - {train_loss}, {train_meas}\")\n",
    "        print(f\"TEST - {test_loss}, {test_meas}\")\n",
    "        print(\"\")\n",
    "        check_meas = test_meas\n",
    "        check_loss = test_loss\n",
    "        check_epoch = epoch\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    print(f\"/*** EPOCH : {epoch+1}/{EPOCH_NUM} ***/\")\n",
    "    print(f\"TRAIN - {train_loss}, {train_meas}\")\n",
    "    print(f\"TEST - {test_loss}, {test_meas}\")\n",
    "    print(\"\")\n",
    "    \n",
    "df_history = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.987Z"
    }
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(2,1,figsize = (8,12))\n",
    "\n",
    "axes[0].plot(df_history['epoch'], df_history['train_loss'])\n",
    "axes[0].plot(df_history['epoch'], df_history['test_loss'])\n",
    "axes[0].axvline(x = check_epoch, ymin = 0, ymax = df_history['test_loss'].max(), color = 'r')\n",
    "axes[0].axhline(y = default_loss, xmin = 0, xmax = df_history['epoch'].max(), color = 'k')\n",
    "\n",
    "axes[1].plot(df_history['epoch'], df_history['train_meas'])\n",
    "axes[1].plot(df_history['epoch'], df_history['test_meas'])\n",
    "axes[1].axvline(x = check_epoch, ymin = 0, ymax = df_history['test_meas'].max(), color = 'r')\n",
    "axes[1].axhline(y = default_meas, xmin = 0, xmax = df_history['epoch'].max(), color = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:36.988Z"
    }
   },
   "outputs": [],
   "source": [
    "del X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.002Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(str(data_dir / \"gsdc_test.pkl.gzip\"))\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.003Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load submission sample\n",
    "submission = pd.read_csv(str(data_dir / \"sample_submission.csv\"))\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.004Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleNetwork(X.shape[1] * X.shape[2], y.shape[1])\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.005Z"
    }
   },
   "outputs": [],
   "source": [
    "X, _ = extract_features(df_test, train = False)\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "\n",
    "loader_test = DataLoader(X, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.006Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()  # 신경망을 추론 모드로 전환\n",
    "\n",
    "# 데이터로더에서 미니배치를 하나씩 꺼내 추론을 수행\n",
    "predict = []\n",
    "with torch.no_grad():  # 추론 과정에는 미분이 필요없음\n",
    "    for data in loader_test:\n",
    "        data = data.to(device)\n",
    "\n",
    "        outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "        predict.append(outputs)\n",
    "predict = torch.cat(predict, axis = 0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.007Z"
    }
   },
   "outputs": [],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.008Z"
    }
   },
   "outputs": [],
   "source": [
    "print(submission.shape)\n",
    "submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.010Z"
    }
   },
   "outputs": [],
   "source": [
    "submission['latDeg'] = predict[:,1]\n",
    "submission['lngDeg'] = predict[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T03:02:37.011Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv(f\"./models/{notebookName}/{num_files} - result.csv\", index = False)\n",
    "pd.DataFrame([]).to_csv(dummy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T01:57:01.692286Z",
     "start_time": "2021-06-19T01:56:30.624Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
